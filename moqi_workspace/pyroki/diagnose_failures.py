"""
è¯Šæ–­IKå¤±è´¥åŸå› 

å¯¹æ¯”æ·»åŠ å·¥ä½œç©ºé—´çº¦æŸå‰åçš„å¤±è´¥æ¨¡å¼
"""

import pandas as pd
import numpy as np
import json
import sys

def analyze_failure_pattern(csv_path, json_path=None):
    """åˆ†æå¤±è´¥æ¨¡å¼"""
    print("=" * 80)
    print("IK å¤±è´¥åŸå› è¯Šæ–­")
    print("=" * 80)
    print(f"\nğŸ“ CSVæ•°æ®: {csv_path}")
    if json_path:
        print(f"ğŸ“ JSONæ•°æ®: {json_path}")
    
    # è¯»å–CSV
    df = pd.read_csv(csv_path)
    df['timestamp'] = pd.to_datetime(df['timestamp'])
    
    total = len(df)
    success = df[df['solved'] == 1]
    failure = df[df['solved'] == 0]
    
    success_count = len(success)
    failure_count = len(failure)
    
    print(f"\nğŸ“Š æ€»ä½“ç»Ÿè®¡")
    print(f"  æ€»å°è¯•:   {total}")
    print(f"  æˆåŠŸ:     {success_count} ({success_count/total*100:.2f}%)")
    print(f"  å¤±è´¥:     {failure_count} ({failure_count/total*100:.2f}%)")
    
    # å¤±è´¥æ—¶é—´åˆ†å¸ƒ
    if failure_count > 0:
        print(f"\nâ±ï¸  å¤±è´¥æ—¶é—´åˆ†å¸ƒ")
        
        # æŒ‰æ—¶é—´çª—å£ç»Ÿè®¡
        df['time_seconds'] = (df['timestamp'] - df['timestamp'].iloc[0]).dt.total_seconds()
        
        # æ‰¾åˆ°å¤±è´¥é›†ä¸­çš„æ—¶é—´æ®µ
        failure_times = df[df['solved'] == 0]['time_seconds'].values
        
        if len(failure_times) > 0:
            # è¯†åˆ«å¤±è´¥ç°‡
            failure_clusters = []
            current_cluster = [failure_times[0]]
            
            for t in failure_times[1:]:
                if t - current_cluster[-1] < 2.0:  # 2ç§’å†…è®¤ä¸ºæ˜¯åŒä¸€ç°‡
                    current_cluster.append(t)
                else:
                    if len(current_cluster) >= 3:  # è‡³å°‘3æ¬¡å¤±è´¥æ‰ç®—ä¸€ä¸ªç°‡
                        failure_clusters.append(current_cluster)
                    current_cluster = [t]
            
            if len(current_cluster) >= 3:
                failure_clusters.append(current_cluster)
            
            print(f"  è¯†åˆ«åˆ° {len(failure_clusters)} ä¸ªå¤±è´¥ç°‡:")
            for i, cluster in enumerate(failure_clusters[:5], 1):  # åªæ˜¾ç¤ºå‰5ä¸ª
                start_time = cluster[0]
                end_time = cluster[-1]
                duration = end_time - start_time
                count = len(cluster)
                print(f"    ç°‡ {i}: {start_time:.1f}s - {end_time:.1f}s (æŒç»­ {duration:.1f}s, {count}æ¬¡å¤±è´¥)")
        
        # åˆ†æå¤±è´¥è€—æ—¶
        fail_times = failure['elapsed_ms']
        print(f"\nâ±ï¸  å¤±è´¥æ±‚è§£è€—æ—¶")
        print(f"  å¹³å‡: {fail_times.mean():.2f} ms")
        print(f"  ä¸­ä½æ•°: {fail_times.median():.2f} ms")
        print(f"  æœ€å¤§: {fail_times.max():.2f} ms")
        
        # è€—æ—¶åˆ†å¸ƒ
        slow_failures = len(fail_times[fail_times > 7])
        print(f"  è€—æ—¶>7msçš„å¤±è´¥: {slow_failures} ({slow_failures/failure_count*100:.1f}%)")
    
    # å¦‚æœæœ‰JSONæ•°æ®ï¼ˆå¤±è´¥è®°å½•ï¼‰ï¼Œåˆ†æç›®æ ‡ä½ç½®
    if json_path:
        try:
            with open(json_path, 'r') as f:
                data = json.load(f)
            
            failures = data['data']
            if len(failures) > 0:
                print(f"\nğŸ¯ å¤±è´¥æ—¶çš„ç›®æ ‡ä½ç½®åˆ†æ ({len(failures)}æ¡è®°å½•)")
                
                # æå–ä½ç½®
                left_positions = np.array([[f[5], f[6], f[7]] for f in failures])
                right_positions = np.array([[f[12], f[13], f[14]] for f in failures])
                
                # å‡è®¾è‚©éƒ¨ä½ç½®ï¼ˆéœ€è¦ä»å®é™…ä»£ç è·å–ï¼‰
                left_shoulder = np.array([0.0, 0.2, 0.5])  # ç¤ºä¾‹å€¼
                right_shoulder = np.array([0.0, -0.2, 0.5])  # ç¤ºä¾‹å€¼
                
                left_distances = np.linalg.norm(left_positions - left_shoulder, axis=1)
                right_distances = np.linalg.norm(right_positions - right_shoulder, axis=1)
                
                max_reach = 0.436  # l1 + l2
                effective_reach = 0.420  # max_reach - safety_margin
                
                print(f"\n  å·¦è‡‚:")
                print(f"    å¹³å‡è·ç¦»: {left_distances.mean():.3f} m")
                print(f"    æœ€å¤§è·ç¦»: {left_distances.max():.3f} m")
                print(f"    æ¥è¿‘è¾¹ç•Œ(>0.40m): {(left_distances > 0.40).sum()} ({(left_distances > 0.40).sum()/len(failures)*100:.1f}%)")
                print(f"    è¶…å‡ºæœ‰æ•ˆåŠå¾„(>0.42m): {(left_distances > effective_reach).sum()}")
                
                print(f"\n  å³è‡‚:")
                print(f"    å¹³å‡è·ç¦»: {right_distances.mean():.3f} m")
                print(f"    æœ€å¤§è·ç¦»: {right_distances.max():.3f} m")
                print(f"    æ¥è¿‘è¾¹ç•Œ(>0.40m): {(right_distances > 0.40).sum()} ({(right_distances > 0.40).sum()/len(failures)*100:.1f}%)")
                print(f"    è¶…å‡ºæœ‰æ•ˆåŠå¾„(>0.42m): {(right_distances > effective_reach).sum()}")
                
                # æ£€æŸ¥æ˜¯å¦åœ¨è¾¹ç•Œä¸Š
                left_at_boundary = (left_distances >= 0.41) & (left_distances <= 0.421)
                right_at_boundary = (right_distances >= 0.41) & (right_distances <= 0.421)
                
                print(f"\n  ğŸ” è¾¹ç•Œå¤±è´¥åˆ†æ:")
                print(f"    å·¦è‡‚åœ¨è¾¹ç•Œ(0.41-0.421m): {left_at_boundary.sum()} ({left_at_boundary.sum()/len(failures)*100:.1f}%)")
                print(f"    å³è‡‚åœ¨è¾¹ç•Œ(0.41-0.421m): {right_at_boundary.sum()} ({right_at_boundary.sum()/len(failures)*100:.1f}%)")
                
                if left_at_boundary.sum() > len(failures) * 0.5 or right_at_boundary.sum() > len(failures) * 0.5:
                    print(f"\n  âš ï¸  è­¦å‘Š: è¶…è¿‡50%çš„å¤±è´¥å‘ç”Ÿåœ¨å·¥ä½œç©ºé—´è¾¹ç•Œ!")
                    print(f"      å¯èƒ½åŸå› : çº¦æŸåçš„è¾¹ç•Œä½ç½®å®¹æ˜“è§¦å‘å…³èŠ‚é™åˆ¶")
                    print(f"      å»ºè®®: å¢å¤§ safety_margin åˆ° 0.020-0.025m")
        
        except Exception as e:
            print(f"\nâš ï¸  æ— æ³•è¯»å–JSON: {e}")
    
    print("\n" + "=" * 80)
    print("è¯Šæ–­å®Œæˆ")
    print("=" * 80 + "\n")


def compare_logs(csv_before, csv_after):
    """å¯¹æ¯”å‰åä¸¤ä¸ªæ—¥å¿—"""
    print("=" * 80)
    print("å¯¹æ¯”åˆ†æ: æ·»åŠ çº¦æŸå‰ vs æ·»åŠ çº¦æŸå")
    print("=" * 80)
    
    df_before = pd.read_csv(csv_before)
    df_after = pd.read_csv(csv_after)
    
    before_total = len(df_before)
    before_success = (df_before['solved'] == 1).sum()
    before_fail = before_total - before_success
    before_rate = before_success / before_total * 100
    
    after_total = len(df_after)
    after_success = (df_after['solved'] == 1).sum()
    after_fail = after_total - after_success
    after_rate = after_success / after_total * 100
    
    print(f"\nğŸ“Š æˆåŠŸç‡å¯¹æ¯”")
    print(f"  æ·»åŠ çº¦æŸå‰: {before_success}/{before_total} = {before_rate:.2f}%")
    print(f"  æ·»åŠ çº¦æŸå: {after_success}/{after_total} = {after_rate:.2f}%")
    print(f"  å˜åŒ–: {after_rate - before_rate:+.2f} ä¸ªç™¾åˆ†ç‚¹")
    
    if after_rate < before_rate:
        print(f"\n  âš ï¸  è­¦å‘Š: æˆåŠŸç‡ä¸‹é™ï¼")
        print(f"  å¯èƒ½åŸå› :")
        print(f"    1. çº¦æŸåçš„è¾¹ç•Œä½ç½®æ›´å®¹æ˜“è§¦å‘å…³èŠ‚é™åˆ¶")
        print(f"    2. safety_margin å¤ªå°ï¼Œå¯¼è‡´è¾¹ç•Œå¥‡å¼‚")
        print(f"    3. çº¦æŸé€»è¾‘å¯èƒ½æœ‰é—®é¢˜")
    else:
        print(f"\n  âœ… æˆåŠŸç‡æå‡!")
    
    print("\n" + "=" * 80 + "\n")


if __name__ == "__main__":
    import sys
    from pathlib import Path
    
    log_dir = Path(__file__).parent / "log"
    
    if len(sys.argv) > 1:
        csv_path = sys.argv[1]
        json_path = sys.argv[2] if len(sys.argv) > 2 else None
    else:
        # ä½¿ç”¨æœ€æ–°çš„æ—¥å¿—
        csv_files = sorted(log_dir.glob("ik_performance_*.csv"), key=lambda p: p.stat().st_mtime)
        if len(csv_files) >= 1:
            csv_path = csv_files[-1]
            
            # å°è¯•æ‰¾JSON
            csv_stem = csv_path.stem.replace('ik_performance_', 'teleop_')
            json_path = log_dir / f"{csv_stem}.json"
            if not json_path.exists():
                json_path = None
        else:
            print("âŒ æœªæ‰¾åˆ°æ—¥å¿—æ–‡ä»¶")
            sys.exit(1)
    
    # åˆ†æå¤±è´¥æ¨¡å¼
    analyze_failure_pattern(csv_path, json_path)
    
    # å¦‚æœæœ‰å¤šä¸ªæ—¥å¿—ï¼Œè¿›è¡Œå¯¹æ¯”
    csv_files = sorted(log_dir.glob("ik_performance_*.csv"), key=lambda p: p.stat().st_mtime)
    if len(csv_files) >= 2:
        print("\n")
        compare_logs(csv_files[-2], csv_files[-1])

